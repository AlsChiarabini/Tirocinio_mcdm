{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60480304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 1 - creo dataframe\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from MJP.majority_portfolio_utilities_TV import * \n",
    "\n",
    "FILENAME = 'monthly_data.csv'\n",
    "factors_df=pd.DataFrame({\n",
    "    'factors': ['ag','beta','bm','cumret','dolvol6','gp','ill6','ns','size','volatility','acc'],\n",
    "    'signs': [  -1.,  1.,   1.,    1.,     -1.,     1.,    1.,  -1.,  -1.,   -1.,   -1.],\n",
    "    'wsigns':[  -1.,  1.,   1.,    1.,     -1.,     1.,    1.,  -1.,  -1.,   -1.,   -1.]\n",
    "})\n",
    "\n",
    "DATA_INIZIO = '2000-01-01'      # In futuro considerare periodi bull e bear\n",
    "DATA_FINE = '2023-12-31'        # Quindi utilizzare sub periodi\n",
    "\n",
    "df = create_df(FILENAME, factors_df['factors'].tolist(), DATA_INIZIO, DATA_FINE) # funzione sua\n",
    "df['date']=pd.to_datetime(df['date']) \n",
    "print(f\"DataFrame caricato: {df.shape[0]:,} righe, {df.shape[1]} colonne\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# PARTE 2 - NORMALIZZO CON QUANTILE NORMALIZER\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "factors_to_normalize = ['ns', 'size', 'ag', 'gp', 'acc', 'bm', 'ill6', 'volatility', 'beta', 'cumret', 'dolvol6']\n",
    "\n",
    "# Elimina righe con NaN solo nelle colonne da normalizzare\n",
    "df_clean = df.dropna(subset=factors_to_normalize)\n",
    "\n",
    "# Applica Quantile normalization solo ai fattori\n",
    "scaler = QuantileTransformer(output_distribution='uniform', random_state=0)\n",
    "df_norm = df_clean.copy()\n",
    "df_norm[factors_to_normalize] = scaler.fit_transform(df_clean[factors_to_normalize])\n",
    "\n",
    "print(\"‚úÖ df normalizzato su fattori:\")\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 3 - inizializzazione della configurazione\n",
    "\n",
    "# *** ATTENZIONE *** --> PRIMA MODIFICA, HO MESSO TRUE A WEIGHT\n",
    "\n",
    "# Lista dei fattori\n",
    "factors_list = factors_df['factors'].to_list()\n",
    "# Segni di default\n",
    "default_signs = factors_df['signs'].to_list()\n",
    "\n",
    "# Dizionario pairwise per AHP/test (tutti a 1.0 per prova)\n",
    "pairwise_dict = {}\n",
    "for i in range(len(factors_list)):\n",
    "    for j in range(i+1, len(factors_list)):\n",
    "        pairwise_dict[(factors_list[i], factors_list[j])] = 1.0  # tutti uguali\n",
    "        # il reciproco viene gestito nella funzione AHP automaticamente\n",
    "\n",
    "remove_outliers = False\n",
    "inf, sup = None, None\n",
    "\n",
    "MJ_configuration = {\n",
    "    'K': 12,  # holding periods in months\n",
    "    'lag': 5,  # 5 per ribilanciamento a giugno\n",
    "    'factors': factors_list,\n",
    "    'default_signs': default_signs,  # 1=beneficio, -1=costo\n",
    "    'num_port': 10,  # number of portfolios\n",
    "    'num_cat': 6,  # number of categories per factor per MJ\n",
    "    'weighting': True,\n",
    "    'verbose': True,\n",
    "    'n_jobs': -1,\n",
    "    'mj_window': 1,\n",
    "    'method': 'majority',  # mean_rank, majority, 75q, 90q, lex, dlex\n",
    "    'rolling_method': 'profile',  # 'rank','vote','profile'\n",
    "    'treat_na_mj': 'median',\n",
    "    'remove_outliers': remove_outliers,\n",
    "    'inclusive': True,\n",
    "    'fix_signs': True,\n",
    "    'all_voters_not_nan_on_reallocation': True,\n",
    "    # Parametri utilizzati solo se fix_signs=False\n",
    "    'min_voters': 5,\n",
    "    'voting_window': 6,\n",
    "    'sign_voting_window': 12,\n",
    "    'p_threshold': 0.1,\n",
    "    'delta_utility': 0,\n",
    "    'eliminations': 1,\n",
    "    'players_batch_size': 5,\n",
    "    'small': True,\n",
    "    # Parametro aggiuntivo per test AHP/EQUALE WEIGHTS\n",
    "    'pairwise': pairwise_dict\n",
    "}\n",
    "\n",
    "if remove_outliers:\n",
    "    MJ_configuration['outliers']=[inf,sup]\n",
    "\n",
    "if MJ_configuration['weighting']:                                                   # CHIEDEREEEEEEEEEEEEEE\n",
    "    MJ_configuration['default_voters']=factors_df['factors'].to_list()\n",
    "    MJ_configuration['default_signs']=factors_df['wsigns'].to_list()\n",
    "    compute='wmj'\n",
    "elif not MJ_configuration['weighting']:\n",
    "    MJ_configuration['default_voters']=factors_df['factors'].to_list()\n",
    "    MJ_configuration['default_signs']=factors_df['signs'].to_list()\n",
    "    compute='mj'\n",
    "\n",
    "factors=factors_df['factors'].to_list() # salva separatamente la lista dei nomi dei fattori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 4 - creazione portafogli single criteria e MJ\n",
    "\n",
    "### COMPUTE SINGLE FACTOR STRATEGIES\n",
    "portfolios, weighted_portfolios, portfolios_stock_reallocation = compute_factor_strategies(df_norm, MJ_configuration)\n",
    "        # '2020-06-30': [list of tickers]       # Con pesi, uguale ma 'AAPL': 0.1           # 'AAPL': 2020-06-30': 'P3', '2021-06-30': 'P5'}\n",
    "        # '2021-06-30': [list of tickers]\n",
    "        # _ perch√© non considero weighted_portfolios al momento\n",
    "\n",
    "### COMPUTE EQUAL WEIGHTED MJ STRATEGY\n",
    "MJ_configuration['weighting']=False\n",
    "MJ_portfolios, \\\n",
    "mj_voters, \\\n",
    "MJ_portfolios_stock_reallocation = compute_MJ_portfolio_strategy(df_norm, MJ_configuration)\n",
    "\n",
    "portfolios['mj'] = MJ_portfolios['mj']\n",
    "portfolios_stock_reallocation['mj'] = {}\n",
    "portfolios_stock_reallocation['mj']['EW_turnover'] = MJ_portfolios_stock_reallocation['mj']\n",
    "\n",
    "### COMPUTE VALUE WEIGHTED MJ STRATEGY\n",
    "MJ_configuration['weighting']=True\n",
    "MJ_weighted_portfolios, \\\n",
    "mj_voters, \\\n",
    "MJ_weighted_portfolios_stock_reallocation = compute_MJ_portfolio_strategy(df_norm, MJ_configuration)\n",
    "\n",
    "weighted_portfolios['wmj'] = MJ_weighted_portfolios['wmj']\n",
    "portfolios_stock_reallocation['wmj'] = {}\n",
    "portfolios_stock_reallocation['wmj']['VW_turnover'] = MJ_weighted_portfolios_stock_reallocation['wmj']\n",
    "\n",
    "'''\n",
    "OUTPUT: \n",
    "portfolios = {\n",
    "  'ns': DataFrame con port1...port10 costruiti su ns,\n",
    "  'beta': ...,\n",
    "  'bm': ...,\n",
    "  ...\n",
    "  'mj': DataFrame con port1...port10 creati con MJ\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cea296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELECTRE TRI-B - VERSIONE COMPLETA E CORRETTA\n",
    "from pyDecision.algorithm import electre_tri_b\n",
    "from tqdm import tqdm\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# CONFIGURAZIONE ELECTRE TRI-B\n",
    "# ------------------------\n",
    "\n",
    "num_factors = len(factors_list)\n",
    "num_port = 10 \n",
    "\n",
    "ELECTRE_configuration = {\n",
    "    'K': 12,\n",
    "    'lag': 5,\n",
    "    'factors': factors_list,\n",
    "    'default_signs': default_signs,\n",
    "    'num_port': num_port,\n",
    "    'weighting': False,\n",
    "    'verbose': True,\n",
    "    \n",
    "    # Parametri ELECTRE Tri-B\n",
    "    'weights': np.ones(num_factors) / num_factors,          # cambia solo se hai motivo solido\n",
    "    'lambda_cut': 0.65,                                      # con molti criteri, dovrebbe stare tra 0.65 < x < 0.75\n",
    "    'q_multiplier': 0.1,                                    # soglia di indifferenza, due stocks vengono considerati uguali (forse 0.2 troppo grande)\n",
    "    'p_multiplier': 0.4,                                    # soglia di preferenza, maggiore di q (0.5 ok)\n",
    "    'v_multiplier': 2.5,                                    # soglia di veto (almeno 3x p), vuol dire che un criterio fa da veto per bloccare relazione di outranking\n",
    "    'num_profiles': num_port - 1,\n",
    "    'rule': 'oc'                                            # 'oc' o 'cc', con oc = outranking congiunto e cc = outranking concordante\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# FUNZIONE ELECTRE TRI-B\n",
    "# ------------------------\n",
    "def compute_electre_iii_rolling_strategy(df: pd.DataFrame, configuration: dict, weighting: bool = False):\n",
    "    \"\"\"\n",
    "    ELECTRE Tri-B implementation for portfolio sorting.\n",
    "    Port 10 = BEST, Port 1 = WORST\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'medate' not in df.columns:\n",
    "        df['medate'] = df['date'] + MonthEnd(0)\n",
    "\n",
    "    # Extract parameters\n",
    "    K = configuration['K']\n",
    "    lag = configuration['lag']\n",
    "    num_port = configuration['num_port']\n",
    "    factors = sorted(configuration['factors'])\n",
    "    types = configuration['default_signs']\n",
    "    verbose = configuration['verbose']\n",
    "    num_profiles = configuration['num_profiles']\n",
    "\n",
    "    weights = np.array(configuration['weights'])\n",
    "    lambda_cut = configuration['lambda_cut']\n",
    "    q_mult = configuration['q_multiplier']\n",
    "    p_mult = configuration['p_multiplier']\n",
    "    v_mult = configuration['v_multiplier']\n",
    "    rule = configuration['rule']\n",
    "    \n",
    "    criteria_type = ['max' if t == 1 else 'min' for t in types]\n",
    "\n",
    "    # Rebalancing dates\n",
    "    reallocation_dates = []\n",
    "    date_start = df['medate'].min() + MonthEnd(lag)\n",
    "    date_end = df['medate'].max()\n",
    "    while date_start + MonthEnd(K) <= date_end:\n",
    "        reallocation_dates.append(date_start)\n",
    "        date_start += MonthEnd(K)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä ELECTRE TRI-B PORTFOLIO STRATEGY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Method: ELECTRE Tri-B with {rule.upper()} assignment rule\")\n",
    "    print(f\"Profiles: {num_profiles} boundary profiles (quantile-based)\")\n",
    "    print(f\"Lambda cut: {lambda_cut} | Thresholds: q={q_mult}œÉ, p={p_mult}œÉ, v={v_mult}œÉ\")\n",
    "    print(f\"Portfolio convention: Port 10 = BEST, Port 1 = WORST\")\n",
    "    print(f\"Rebalancing dates: {len(reallocation_dates)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Initialize\n",
    "    returns_dict = {f'port{i+1}': [] for i in range(num_port)}\n",
    "    returns_dict['long_short'] = []\n",
    "    turnover_values = {f'port{i+1}': [] for i in range(num_port)}\n",
    "    reallocation = {f'port{i+1}': {} for i in range(num_port)}\n",
    "    prev_portfolios = {f'port{i+1}': None for i in range(num_port)}\n",
    "    \n",
    "    labels = [f'port{i+1}' for i in range(num_port)]\n",
    "\n",
    "    # Statistics tracking\n",
    "    total_assets = 0\n",
    "    category_counts = {i: 0 for i in range(num_profiles + 1)}  # 0 to num_profiles\n",
    "\n",
    "    # Rolling window loop\n",
    "    iteration = 0\n",
    "    for date in tqdm(reallocation_dates, desc=\"ELECTRE Tri-B rolling\", disable=not verbose):\n",
    "        iteration += 1\n",
    "        \n",
    "        df_now = df[df['medate'] == date].dropna(subset=factors).copy()\n",
    "        \n",
    "        if df_now.empty or len(df_now) < num_port:\n",
    "            continue\n",
    "\n",
    "        matrix = df_now[factors].values\n",
    "        total_assets += len(df_now)\n",
    "        \n",
    "        # Define boundary profiles using quantiles\n",
    "        percentiles = np.linspace(1.0/(num_profiles+1), num_profiles/(num_profiles+1), num_profiles)\n",
    "        profiles_list = []\n",
    "        for p in percentiles:\n",
    "            row_profile = df_now[factors].quantile(p).values \n",
    "            profiles_list.append(row_profile)\n",
    "        profiles = np.array(profiles_list)\n",
    "\n",
    "        # Compute dynamic thresholds\n",
    "        std_devs = df_now[factors].std().values\n",
    "        if std_devs.ndim > 1:\n",
    "            std_devs = std_devs.flatten()\n",
    "\n",
    "        q_array = q_mult * std_devs\n",
    "        p_array = p_mult * std_devs\n",
    "        v_array = v_mult * std_devs\n",
    "\n",
    "        q_vec = [float(x) for x in q_array]\n",
    "        p_vec = [float(x) for x in p_array]\n",
    "        v_vec = [float(x) for x in v_array]\n",
    "\n",
    "        # Handle criteria direction\n",
    "        matrix_adjusted = matrix.copy()\n",
    "        profiles_adjusted = profiles.copy()\n",
    "        \n",
    "        for idx, crit_type in enumerate(criteria_type):\n",
    "            if crit_type == 'min':\n",
    "                matrix_adjusted[:, idx] = -matrix_adjusted[:, idx]\n",
    "                profiles_adjusted[:, idx] = -profiles_adjusted[:, idx]\n",
    "\n",
    "        # Execute ELECTRE Tri-B\n",
    "        try:\n",
    "            assignments = electre_tri_b(\n",
    "                matrix_adjusted,\n",
    "                weights.tolist(),\n",
    "                q_vec,\n",
    "                p_vec,\n",
    "                v_vec,\n",
    "                profiles_adjusted.tolist(),\n",
    "                lambda_cut,\n",
    "                False,\n",
    "                rule\n",
    "            )\n",
    "            \n",
    "            # Track category distribution\n",
    "            unique_cats, counts = np.unique(assignments, return_counts=True)\n",
    "            for cat, count in zip(unique_cats, counts):\n",
    "                category_counts[int(cat)] += count\n",
    "            \n",
    "            if iteration == 1:\n",
    "                print(f\"First iteration ({date}):\")\n",
    "                print(f\"  Assets: {len(df_now)}\")\n",
    "                print(f\"  Category distribution:\")\n",
    "                for cat, count in zip(unique_cats, counts):\n",
    "                    pct = count/len(assignments)*100\n",
    "                    print(f\"    Category {int(cat)}: {count} assets ({pct:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ELECTRE Tri-B error at {date}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df_now['category'] = assignments\n",
    "\n",
    "        # ‚úÖ MAPPATURA CORRETTA\n",
    "        # ELECTRE Tri-B con 9 profili restituisce categorie: 0, 1, 2, ..., 9\n",
    "        # Mappatura: Cat 0 ‚Üí Port 1 (worst), Cat 1 ‚Üí Port 2, ..., Cat 9 ‚Üí Port 10 (best)\n",
    "        \n",
    "        def map_category_to_portfolio(cat):\n",
    "            if pd.isna(cat):\n",
    "                return np.nan\n",
    "            \n",
    "            cat_int = int(cat)\n",
    "            \n",
    "            # Category 0 (rejected) ‚Üí Port 1 (worst)\n",
    "            if cat_int == 0:\n",
    "                return 'port1'\n",
    "            \n",
    "            # Valid categories: 1 to num_profiles (1 to 9)\n",
    "            if cat_int < 1 or cat_int > num_profiles:\n",
    "                return np.nan\n",
    "            \n",
    "            # Map: Cat 1 ‚Üí Port 2, Cat 2 ‚Üí Port 3, ..., Cat 9 ‚Üí Port 10\n",
    "            port_num = cat_int + 1\n",
    "            return f'port{port_num}'\n",
    "\n",
    "        df_now['portfolio'] = df_now['category'].apply(map_category_to_portfolio)\n",
    "        \n",
    "        if iteration == 1:\n",
    "            print(f\"\\n  Portfolio assignment:\")\n",
    "            for port in labels:\n",
    "                count = len(df_now[df_now['portfolio'] == port])\n",
    "                if count > 0:\n",
    "                    print(f\"    {port}: {count} assets\")\n",
    "            print()\n",
    "        \n",
    "        df_now = df_now.dropna(subset=['portfolio'])\n",
    "        \n",
    "        if df_now.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute returns\n",
    "        period_end = date + MonthEnd(K)\n",
    "        df_hold = df[(df['medate'] > date) & (df['medate'] <= period_end)]\n",
    "\n",
    "        port_permnos = {}\n",
    "        for port in labels:\n",
    "            tickers = df_now[df_now['portfolio'] == port]['PERMNO'].tolist()\n",
    "            port_permnos[port] = tickers\n",
    "            reallocation[port][date] = tickers\n",
    "            \n",
    "            df_port = df_hold[df_hold['PERMNO'].isin(tickers)].copy()\n",
    "            \n",
    "            if df_port.empty:\n",
    "                continue\n",
    "            \n",
    "            if weighting:\n",
    "                df_port['wret'] = df_port['RET_RF'] * df_port['me_lag']\n",
    "                wret_sum = df_port.groupby('medate')['wret'].sum()\n",
    "                me_sum = df_port.groupby('medate')['me_lag'].sum()\n",
    "                ret = (wret_sum / me_sum).dropna()\n",
    "            else:\n",
    "                ret = df_port.groupby('medate')['RET_RF'].mean()\n",
    "            \n",
    "            if isinstance(ret, pd.DataFrame):\n",
    "                ret = ret.squeeze()\n",
    "            \n",
    "            if len(ret) > 0:\n",
    "                returns_dict[port].append(ret)\n",
    "\n",
    "        # Turnover\n",
    "        for port in labels:\n",
    "            curr_holdings = set(port_permnos[port])\n",
    "            if prev_portfolios[port] is not None:\n",
    "                prev_holdings = set(prev_portfolios[port])\n",
    "                intersection = len(prev_holdings & curr_holdings)\n",
    "                total_unique = max(len(prev_holdings), len(curr_holdings))\n",
    "                turnover_rate = 1 - (intersection / total_unique) if total_unique > 0 else 0.0\n",
    "            else:\n",
    "                turnover_rate = np.nan\n",
    "            turnover_values[port].append(turnover_rate)\n",
    "            prev_portfolios[port] = port_permnos[port]\n",
    "\n",
    "        # Long-short: Port 10 (best) LONG, Port 1 (worst) SHORT\n",
    "        long = df_hold[df_hold['PERMNO'].isin(port_permnos['port10'])].copy()\n",
    "        short = df_hold[df_hold['PERMNO'].isin(port_permnos['port1'])].copy()\n",
    "\n",
    "        if not long.empty and not short.empty:\n",
    "            if weighting:\n",
    "                long['wret'] = long['RET_RF'] * long['me_lag']\n",
    "                short['wret'] = short['RET_RF'] * short['me_lag']\n",
    "                \n",
    "                long_wret_sum = long.groupby('medate')['wret'].sum()\n",
    "                long_me_sum = long.groupby('medate')['me_lag'].sum()\n",
    "                ret_long = (long_wret_sum / long_me_sum).dropna()\n",
    "                \n",
    "                short_wret_sum = short.groupby('medate')['wret'].sum()\n",
    "                short_me_sum = short.groupby('medate')['me_lag'].sum()\n",
    "                ret_short = (short_wret_sum / short_me_sum).dropna()\n",
    "            else:\n",
    "                ret_long = long.groupby('medate')['RET_RF'].mean()\n",
    "                ret_short = short.groupby('medate')['RET_RF'].mean()\n",
    "            \n",
    "            ls_ret = (ret_long - ret_short).dropna()\n",
    "            \n",
    "            if isinstance(ls_ret, pd.DataFrame):\n",
    "                ls_ret = ls_ret.squeeze()\n",
    "            \n",
    "            if len(ls_ret) > 0:\n",
    "                returns_dict['long_short'].append(ls_ret)\n",
    "\n",
    "    # Final statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä ELECTRE TRI-B CLASSIFICATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total assets classified: {total_assets:,}\")\n",
    "    print(f\"\\nCategory distribution across all periods:\")\n",
    "    for cat in sorted(category_counts.keys()):\n",
    "        count = category_counts[cat]\n",
    "        if count > 0:\n",
    "            pct = count / total_assets * 100\n",
    "            cat_label = \"Rejected\" if cat == 0 else f\"Cat {cat}\"\n",
    "            print(f\"  {cat_label}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    rejected_pct = category_counts[0] / total_assets * 100 if total_assets > 0 else 0\n",
    "    print(f\"\\n‚ö†Ô∏è  Rejection rate (Category 0 ‚Üí Port 1): {rejected_pct:.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Combine results\n",
    "    for key in returns_dict:\n",
    "        if returns_dict[key]:\n",
    "            concatenated = pd.concat(returns_dict[key], axis=0)\n",
    "            if isinstance(concatenated, pd.DataFrame):\n",
    "                concatenated = concatenated.squeeze()\n",
    "            returns_dict[key] = concatenated.sort_index()\n",
    "        else:\n",
    "            returns_dict[key] = pd.Series(dtype=float)\n",
    "\n",
    "    portfolios_df = pd.DataFrame(returns_dict)\n",
    "    \n",
    "    print(f\"üìà Portfolio returns:\")\n",
    "    print(f\"  Shape: {portfolios_df.shape}\")\n",
    "    print(f\"  Date range: {portfolios_df.index.min()} to {portfolios_df.index.max()}\")\n",
    "    \n",
    "    nan_counts = portfolios_df.isna().sum()\n",
    "    if nan_counts.sum() > 0:\n",
    "        print(f\"  Missing values per portfolio:\")\n",
    "        for col in portfolios_df.columns:\n",
    "            if nan_counts[col] > 0:\n",
    "                print(f\"    {col}: {nan_counts[col]} NaN\")\n",
    "    \n",
    "    portfolios_df = portfolios_df.dropna(thresh=num_port//2)\n",
    "    print(f\"  After filtering: {portfolios_df.shape}\\n\")\n",
    "    \n",
    "    avg_turnover = {port: np.nanmean(values) if values else np.nan \n",
    "                    for port, values in turnover_values.items()}\n",
    "    turnover_key = 'VW_turnover' if weighting else 'EW_turnover'\n",
    "    \n",
    "    turnover_df = pd.DataFrame({\n",
    "        'portfolio': [int(p.replace('port','')) for p in avg_turnover.keys()],\n",
    "        turnover_key: list(avg_turnover.values())\n",
    "    })\n",
    "    \n",
    "    portfolios_stock_reallocation = {\n",
    "        turnover_key: turnover_df,\n",
    "        'reallocation': reallocation\n",
    "    }\n",
    "\n",
    "    return portfolios_df, portfolios_stock_reallocation\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# ESECUZIONE\n",
    "# ------------------------\n",
    "ELECTRE_portfolios, ELECTRE_portfolios_stock_reallocation = compute_electre_iii_rolling_strategy(\n",
    "    df_norm, ELECTRE_configuration, weighting=False\n",
    ") \n",
    "\n",
    "weighted_ELECTRE_portfolios, weighted_ELECTRE_portfolios_stock_reallocation = compute_electre_iii_rolling_strategy(\n",
    "    df_norm, ELECTRE_configuration, weighting=True\n",
    ")\n",
    "\n",
    "portfolios['electre'] = ELECTRE_portfolios \n",
    "portfolios_stock_reallocation['electre'] = ELECTRE_portfolios_stock_reallocation \n",
    "weighted_portfolios['welectre'] = weighted_ELECTRE_portfolios \n",
    "portfolios_stock_reallocation['welectre'] = weighted_ELECTRE_portfolios_stock_reallocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17000fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# *************************************************************\n",
    "# Questo per controllare che effettivamente electre funzioni **\n",
    "# *************************************************************\n",
    "\n",
    "## 1. Verifica del DataFrame dei Rendimenti (Risultato Principale)\n",
    "print(\"--- üìä Verifica Rendimenti (Prime 5 Righe) ---\")\n",
    "print(ELECTRE_portfolios.head())\n",
    "\n",
    "print(\"\\n--- üìà Rendimenti Medi e Conteggio NaN per Portafoglio ---\")\n",
    "# Calcola la media e il numero di osservazioni non nulle\n",
    "rendimenti_verificati = ELECTRE_portfolios.agg(['mean', 'count'])\n",
    "print(rendimenti_verificati)\n",
    "\n",
    "# ---\n",
    "print(\"\\n--- üì¶ Verifica Composizione (Ribilanciamento) ---\")\n",
    "# Estrai il dizionario di ribilanciamento\n",
    "reallocation_data = ELECTRE_portfolios_stock_reallocation['reallocation']\n",
    "\n",
    "if not reallocation_data:\n",
    "    print(\"ATTENZIONE: Il dizionario di ribilanciamento √® vuoto. La computazione potrebbe non aver trovato abbastanza dati in nessuna finestra.\")\n",
    "else:\n",
    "    # Prendi la prima data di ribilanciamento disponibile\n",
    "    first_date = next(iter(reallocation_data['port1'].keys()))\n",
    "    \n",
    "    print(f\"\\nData di Ribilanciamento Controllata: {first_date}\")\n",
    "    \n",
    "    # 2. Verifica del Portafoglio Migliore (port1)\n",
    "    port1_holdings = reallocation_data['port1'][first_date]\n",
    "    print(f\"Portafoglio Long (Port1) - Totale Assets: {len(port1_holdings)}\")\n",
    "    print(f\"Primi 5 Tickers: {port1_holdings[:5]}...\")\n",
    "    \n",
    "    # 3. Verifica del Portafoglio Peggiore (portN)\n",
    "    portN = f'port{ELECTRE_configuration[\"num_port\"]}'\n",
    "    portN_holdings = reallocation_data[portN][first_date]\n",
    "    print(f\"Portafoglio Short ({portN}) - Totale Assets: {len(portN_holdings)}\")\n",
    "    print(f\"Primi 5 Tickers: {portN_holdings[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== FACCIO I PORTAFOGLI ==========================\n",
    "\n",
    "# PARTE 6 - COMPUTE EW-VW MCDM STRATEGY\n",
    "\n",
    "# --- EW (Equal Weight) ---\n",
    "from tqdm import tqdm\n",
    "\n",
    "TOPSIS_portfolios, TOPSIS_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'topsis', MJ_configuration, weighting=False)\n",
    "VIKOR_portfolios, VIKOR_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'vikor', MJ_configuration, weighting=False)\n",
    "PROMETHEE_portfolios, PROMETHEE_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'promethee', MJ_configuration, weighting=False)\n",
    "AHP_portfolios, AHP_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'ahp', MJ_configuration, weighting=False)\n",
    "\n",
    "# inserisco EW mcdm in portfolios e reallocation\n",
    "portfolios['topsis'] = TOPSIS_portfolios\n",
    "portfolios['vikor'] = VIKOR_portfolios\n",
    "portfolios['promethee'] = PROMETHEE_portfolios\n",
    "portfolios['ahp'] = AHP_portfolios\n",
    "\n",
    "portfolios_stock_reallocation['topsis'] = TOPSIS_portfolios_stock_reallocation\n",
    "portfolios_stock_reallocation['vikor'] = VIKOR_portfolios_stock_reallocation\n",
    "portfolios_stock_reallocation['promethee'] = PROMETHEE_portfolios_stock_reallocation\n",
    "portfolios_stock_reallocation['ahp'] = AHP_portfolios_stock_reallocation\n",
    "\n",
    "# --- VW (Value Weighted) ---\n",
    "TOPSIS_weighted_portfolios, TOPSIS_weighted_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'topsis', MJ_configuration, weighting=True)\n",
    "VIKOR_weighted_portfolios, VIKOR_weighted_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'vikor', MJ_configuration, weighting=True)\n",
    "PROMETHEE_weighted_portfolios, PROMETHEE_weighted_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'promethee', MJ_configuration, weighting=True)\n",
    "AHP_weighted_portfolios, AHP_weighted_portfolios_stock_reallocation = compute_mcdm_rolling_strategy(df_norm, 'ahp', MJ_configuration, weighting=True)\n",
    "\n",
    "# inserisco VW mcdm in portfolios e reallocation\n",
    "weighted_portfolios['wtopsis'] = TOPSIS_weighted_portfolios\n",
    "weighted_portfolios['wvikor'] = VIKOR_weighted_portfolios\n",
    "weighted_portfolios['wpromethee'] = PROMETHEE_weighted_portfolios\n",
    "weighted_portfolios['wahp'] = AHP_weighted_portfolios\n",
    "\n",
    "portfolios_stock_reallocation['wtopsis'] = TOPSIS_weighted_portfolios_stock_reallocation\n",
    "portfolios_stock_reallocation['wvikor'] = VIKOR_weighted_portfolios_stock_reallocation\n",
    "portfolios_stock_reallocation['wpromethee'] = PROMETHEE_weighted_portfolios_stock_reallocation\n",
    "portfolios_stock_reallocation['wahp'] = AHP_weighted_portfolios_stock_reallocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************************************************************************\n",
    "# Questo per vedere quale portafoglio funzioni meglio solo con rendimento (P10 - EW) **\n",
    "# *************************************************************************************\n",
    "\n",
    "metodi = list(portfolios.keys())                                                    ## Qua modificare per vedere i vari cosi da mettere nella tabella\n",
    "\n",
    "best_portfolios = {}                                                                                                              \n",
    "\n",
    "print(\"Studio dei rendimenti dei portafogli *port10* (EW) per ogni metodo:\")\n",
    "for metodo in metodi:\n",
    "    df_metodo = portfolios[metodo]\n",
    "\n",
    "    if 'port10' not in df_metodo.columns:\n",
    "        print(f\"‚ö†Ô∏è Attenzione: {metodo} non ha port10.\")\n",
    "        continue\n",
    "\n",
    "    rendimento_port10 = df_metodo['port10'].mean()\n",
    "\n",
    "    best_portfolios[metodo] = {\n",
    "        'portafoglio': 'port10',\n",
    "        'rendimento_medio': rendimento_port10\n",
    "    }\n",
    "\n",
    "for metodo, info in best_portfolios.items():\n",
    "    print(f\"{metodo.upper():<10} ‚ûú rendimento medio mensile EW_P10 = {info['rendimento_medio']:.4f}\")\n",
    "\n",
    "# Trova il metodo il cui port10 ha avuto il rendimento medio pi√π alto\n",
    "migliore_assoluto = max(best_portfolios.items(), key=lambda x: x[1]['rendimento_medio'])\n",
    "\n",
    "metodo_top = migliore_assoluto[0]\n",
    "rendimento_top = migliore_assoluto[1]['rendimento_medio']\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(f\"WINNER: Il metodo con il miglior PORT10 √®: {metodo_top.upper()} con rendimento medio mensile di {rendimento_top:.4f}\")\n",
    "print(\"*\" * 50)\n",
    "\n",
    "metodi_w = list(weighted_portfolios.keys())\n",
    "\n",
    "# *************************************************************************************\n",
    "# Questo per vedere quale portafoglio funzioni meglio solo con rendimento (P10 - VW) **\n",
    "# *************************************************************************************\n",
    "\n",
    "best_wportfolios = {}                                                                                                                  \n",
    "\n",
    "print(\"Studio dei rendimenti dei portafogli *port10* (VW) per ogni metodo:\")\n",
    "for metodo in metodi_w:\n",
    "    df_metodo = weighted_portfolios[metodo]\n",
    "\n",
    "    if 'port10' not in df_metodo.columns:\n",
    "        print(f\"‚ö†Ô∏è Attenzione: {metodo} non ha port10.\")\n",
    "        continue\n",
    "\n",
    "    rendimento_port10 = df_metodo['port10'].mean()\n",
    "\n",
    "    best_wportfolios[metodo] = {\n",
    "        'portafoglio': 'port10',\n",
    "        'rendimento_medio': rendimento_port10\n",
    "    }\n",
    "\n",
    "for metodo, info in best_wportfolios.items():\n",
    "    print(f\"{metodo.upper():<10} ‚ûú rendimento medio mensile VW_P10 = {info['rendimento_medio']:.4f}\")\n",
    "\n",
    "# Trova il metodo il cui port10 ha avuto il rendimento medio pi√π alto\n",
    "migliore_assoluto = max(best_wportfolios.items(), key=lambda x: x[1]['rendimento_medio'])\n",
    "\n",
    "metodo_top = migliore_assoluto[0]\n",
    "rendimento_top = migliore_assoluto[1]['rendimento_medio']\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(f\"WINNER: Il metodo con il miglior PORT10 (VW) √®: {metodo_top.upper()} con rendimento medio mensile di {rendimento_top:.4f}\")\n",
    "print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************************************************\n",
    "# Questo per vedere quale portafoglio funzioni meglio solo con rendimento (LS - EW) **\n",
    "# ************************************************************************************\n",
    "\n",
    "metodi = list(portfolios.keys())\n",
    "\n",
    "best_portfolios_LS = {}                                                                                                                   #    <--------------------------------\n",
    "\n",
    "print(\"Studio dei rendimenti dei portafogli *long_short* (EW) per ogni metodo:\")\n",
    "for metodo in metodi:\n",
    "    df_metodo = portfolios[metodo]\n",
    "\n",
    "    if 'long_short' not in df_metodo.columns:\n",
    "        print(f\"‚ö†Ô∏è Attenzione: {metodo} non ha long_short.\")\n",
    "        continue\n",
    "\n",
    "    rendimento_LS = df_metodo['long_short'].mean()\n",
    "\n",
    "    best_portfolios_LS[metodo] = {\n",
    "        'portafoglio': 'long_short',\n",
    "        'rendimento_medio': rendimento_LS\n",
    "    }\n",
    "\n",
    "for metodo, info in best_portfolios_LS.items():\n",
    "    print(f\"{metodo.upper():<10} ‚ûú rendimento medio mensile EW_LS = {info['rendimento_medio']:.4f}\")\n",
    "\n",
    "# Trova il metodo il cui LS ha avuto il rendimento medio pi√π alto\n",
    "migliore_assoluto_LS = max(best_portfolios_LS.items(), key=lambda x: x[1]['rendimento_medio'])\n",
    "\n",
    "metodo_top_LS = migliore_assoluto_LS[0]\n",
    "rendimento_top_LS = migliore_assoluto_LS[1]['rendimento_medio']\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(f\"WINNER: Il metodo con il miglior longshort √®: {metodo_top_LS.upper()} con rendimento medio mensile di {rendimento_top_LS:.4f}\")\n",
    "print(\"*\" * 50)\n",
    "\n",
    "# UGUALE MA CON LONG SHORT\n",
    "metodi_w = list(weighted_portfolios.keys())\n",
    "\n",
    "# *************************************************************************************\n",
    "# Questo per vedere quale portafoglio funzioni meglio solo con rendimento (LS - EW) **\n",
    "# *************************************************************************************\n",
    "\n",
    "best_wportfolios_WLS = {}                                                                                                                   #    <--------------------------------\n",
    "\n",
    "print(\"Studio dei rendimenti dei portafogli *long_short* (VW) per ogni metodo:\")\n",
    "for metodo in metodi_w:\n",
    "    df_metodo = weighted_portfolios[metodo]\n",
    "\n",
    "    if 'long_short' not in df_metodo.columns:\n",
    "        print(f\"‚ö†Ô∏è Attenzione: {metodo} non ha long_short.\")\n",
    "        continue\n",
    "\n",
    "    rendimento_WLS = df_metodo['long_short'].mean()\n",
    "\n",
    "    best_wportfolios_WLS[metodo] = {\n",
    "        'portafoglio': 'long_short',\n",
    "        'rendimento_medio': rendimento_WLS\n",
    "    }\n",
    "\n",
    "for metodo, info in best_wportfolios_WLS.items():\n",
    "    print(f\"{metodo.upper():<10} ‚ûú rendimento medio mensile VW_LS = {info['rendimento_medio']:.4f}\")\n",
    "\n",
    "# Trova il metodo il cui LS ha avuto il rendimento medio pi√π alto\n",
    "migliore_assoluto_WLS = max(best_wportfolios_WLS.items(), key=lambda x: x[1]['rendimento_medio'])\n",
    "\n",
    "metodo_top_WLS = migliore_assoluto_WLS[0]\n",
    "rendimento_top_WLS = migliore_assoluto_WLS[1]['rendimento_medio']\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(f\"WINNER: Il metodo con il miglior longshort √®: {metodo_top_WLS.upper()} con rendimento medio mensile di {rendimento_top_WLS:.4f}\")\n",
    "print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== STATISTICHE PORTAFOGLI (TUTTI) ==========================\n",
    "\n",
    "def calculate_sharpe_ratio(returns_series, risk_free_rate=0.0):\n",
    "\n",
    "    '''\n",
    "    Calcola lo Sharpe Ratio annualizzato.\n",
    "    Assume returns_series sono rendimenti mensili.\n",
    "    '''\n",
    "\n",
    "    mean_return = returns_series.mean() * 12 # Rendimento medio annualizzato\n",
    "    std_dev = returns_series.std(ddof=1) * np.sqrt(12) # Deviazione standard campionaria per riproducibilita\n",
    "\n",
    "    annualized_risk_free_rate = risk_free_rate * 12\n",
    "\n",
    "    if std_dev == 0:\n",
    "        return np.nan\n",
    "    return (mean_return - annualized_risk_free_rate) / std_dev\n",
    "\n",
    "def calculate_sortino_ratio(returns_series, mar_monthly: float = 0.0):\n",
    "    \"\"\"\n",
    "    Sortino Ratio annualizzato.\n",
    "    - returns_series: rendimenti MENSILI\n",
    "    - mar_monthly: Minimum Acceptable Return MENSILE (default 0.0). \n",
    "      Se vuoi usarlo come risk-free mensile, passalo qui.\n",
    "    \"\"\"\n",
    "    # Numeratore: extra-mean rispetto al MAR, annualizzato\n",
    "    excess_mean_ann = (returns_series.mean() - mar_monthly) * 12\n",
    "\n",
    "    # Downside deviation mensile: sqrt( mean( min(0, r - MAR)^2 ) )\n",
    "    downside_diff = np.minimum(0.0, returns_series - mar_monthly)\n",
    "    downside_dev_monthly = np.sqrt((downside_diff ** 2).mean())\n",
    "\n",
    "    # Annualizzazione della downside deviation\n",
    "    downside_dev_ann = downside_dev_monthly * np.sqrt(12)\n",
    "\n",
    "    if downside_dev_ann == 0 or np.isnan(downside_dev_ann):\n",
    "        return np.nan\n",
    "    return excess_mean_ann / downside_dev_ann\n",
    "\n",
    "def compute_portfolio_stats(best_portfolios, portfolios_dict, mar_monthly: float = 0.0):\n",
    "    risultati = []\n",
    "\n",
    "    for metodo, info in best_portfolios.items():\n",
    "        portafoglio = info['portafoglio']\n",
    "        serie = portfolios_dict[metodo][portafoglio]\n",
    "\n",
    "        rendimento_medio = serie.mean()\n",
    "        volatilita = serie.std()\n",
    "        sharpe = calculate_sharpe_ratio(serie)  # usa risk_free mensile = 0.0 di default\n",
    "        sortino = calculate_sortino_ratio(serie, mar_monthly=mar_monthly)\n",
    "\n",
    "        # Calcolo drawdown\n",
    "        cumulative = (1 + serie).cumprod()\n",
    "        peak = cumulative.cummax()\n",
    "        drawdown = (cumulative - peak) / peak\n",
    "        max_drawdown = abs(drawdown.min())\n",
    "\n",
    "        risultati.append({\n",
    "            'Metodo': metodo,\n",
    "            'Portafoglio': portafoglio,\n",
    "            'Rendimento medio': rendimento_medio,\n",
    "            'Volatilit√†': volatilita,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Sortino Ratio': sortino,\n",
    "            'Max Drawdown': max_drawdown\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(risultati)\n",
    "\n",
    "# **********************************************************************\n",
    "# Tutte le caratteristiche di tutti i portafogli (qua P10 - EW e VW) **\n",
    "# **********************************************************************\n",
    "\n",
    "print(\"\\n--- Statistiche dei portafogli basandosi su P10 ---\")\n",
    "stats_ew = compute_portfolio_stats(best_portfolios, portfolios)\n",
    "stats_vw = compute_portfolio_stats(best_wportfolios, weighted_portfolios)\n",
    "\n",
    "print(\"Statistiche portafogli Equal Weighted (EW):\")\n",
    "print(stats_ew.round(4))\n",
    "print(\"\\nStatistiche portafogli Value Weighted (VW):\")\n",
    "print(stats_vw.round(4))\n",
    "\n",
    "# *********************************************************************\n",
    "# Tutte le caratteristiche di tutti i portafogli (qua LS - EW e VW) **\n",
    "# *********************************************************************\n",
    "\n",
    "print(\"\\n--- Statistiche dei portafogli basandosi su LS ---\")\n",
    "stats_ew_LS = compute_portfolio_stats(best_portfolios_LS, portfolios)\n",
    "stats_vw_LS = compute_portfolio_stats(best_wportfolios_WLS, weighted_portfolios)\n",
    "\n",
    "print(\"Statistiche portafogli Equal Weighted (EW):\")\n",
    "print(stats_ew_LS.round(4))\n",
    "print(\"\\nStatistiche portafogli Value Weighted (VW):\")\n",
    "print(stats_vw_LS.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e94714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== TEST STATISTICI SU SHARPE - BOOTSTRAP A BLOCCHI ==========================\n",
    "\n",
    "# **********************************************************************\n",
    "# Cambiato da bootstrap a blocchi --> versione di Romano\n",
    "# Bootstrap = singola stat, senza confronti multipli\n",
    "# Romano = confronti multipli, controlla il data snooping (modificare i dati artificalmente per avere risultati statistici ok)\n",
    "#           Robusto a non-normalita', standard nella letteratura\n",
    "# **********************************************************************\n",
    "\n",
    "def bootstrap_sharpe_comparison_block(returns1, returns2, num_bootstrap_samples=2000,\n",
    "\n",
    "    block_size=6, risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "\n",
    "    Confronta gli Sharpe Ratio di due serie di rendimenti usando il block bootstrap.\n",
    "\n",
    "    Restituisce la proporzione di volte che Sharpe1 > Sharpe2 (p-value unilaterale).\n",
    "\n",
    "    Args:\n",
    "\n",
    "    returns1 (pd.Series): Serie di rendimenti del primo portafoglio.\n",
    "\n",
    "    returns2 (pd.Series): Serie di rendimenti del secondo portafoglio.\n",
    "\n",
    "    num_bootstrap_samples (int): Numero di simulazioni bootstrap da eseguire.\n",
    "\n",
    "    block_size (int): La dimensione del blocco di rendimenti da campionare.\n",
    "\n",
    "    risk_free_rate (float): Tasso di rendimento risk-free mensile.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sharpes_diff = []\n",
    "    n = len(returns1)\n",
    "\n",
    "    # Calcola il numero di blocchi disponibili\n",
    "    num_blocks = n // block_size\n",
    "    if num_blocks == 0:\n",
    "        print(\"Errore: la dimensione del blocco √® maggiore della lunghezza dei dati.\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "\n",
    "    # *** MODIFICA: Campionamento con block bootstrap ***\n",
    "    # 1. Campiona gli INDICI dei blocchi con reinserimento\n",
    "        block_indices = np.random.choice(num_blocks, size=num_blocks, replace=True)\n",
    "\n",
    "    # 2. Ricostruisci le serie di rendimenti concatenando i blocchi campionati\n",
    "\n",
    "        sample1 = np.empty(0)\n",
    "        sample2 = np.empty(0)\n",
    "        for idx in block_indices:\n",
    "            start_idx = idx * block_size\n",
    "            end_idx = start_idx + block_size\n",
    "            sample1 = np.concatenate([sample1, returns1.iloc[start_idx:end_idx]])\n",
    "            sample2 = np.concatenate([sample2, returns2.iloc[start_idx:end_idx]])\n",
    "\n",
    "        # Assicura che la lunghezza del campione sia esattamente la stessa dell'originale\n",
    "        sample1 = sample1[:n]\n",
    "        sample2 = sample2[:n]\n",
    "\n",
    "        # *** FINE MODIFICA ***\n",
    "\n",
    "        sharpe1 = calculate_sharpe_ratio(pd.Series(sample1), risk_free_rate)\n",
    "        sharpe2 = calculate_sharpe_ratio(pd.Series(sample2), risk_free_rate)\n",
    "\n",
    "\n",
    "        if not np.isnan(sharpe1) and not np.isnan(sharpe2):\n",
    "            sharpes_diff.append(sharpe1 - sharpe2)\n",
    "\n",
    "\n",
    "    if not sharpes_diff:\n",
    "        return np.nan\n",
    "\n",
    "    p_value_one_sided = np.sum(np.array(sharpes_diff) <= 0) / len(sharpes_diff)\n",
    "\n",
    "    return p_value_one_sided\n",
    "\n",
    "'''\n",
    "Bootstrap di Romano (stazionario) \n",
    "'''\n",
    "# Con questa parte genero una serie temporale bootstrap (blocchi lunghezza casuale, mantiene autocorrelazione (ok letteratura))\n",
    "def stationary_bootstrap_series(series, p):\n",
    "    \"\"\"\n",
    "    Stationary Bootstrap (Politis & Romano, 1994)\n",
    "    - p = probability of starting a new block (equiv. 1/block_size)\n",
    "    \"\"\"\n",
    "    n = len(series)\n",
    "    indices = np.zeros(n, dtype=int)\n",
    "\n",
    "    # 1. Scegli un punto di partenza a caso\n",
    "    indices[0] = np.random.randint(0, n)\n",
    "\n",
    "    # 2. Costruisci gli indici uno per uno\n",
    "    for t in range(1, n):\n",
    "        if np.random.rand() < p:\n",
    "            # Inizio un nuovo blocco\n",
    "            indices[t] = np.random.randint(0, n)\n",
    "        else:\n",
    "            # Continuo il blocco precedente (circolare)\n",
    "            indices[t] = (indices[t-1] + 1) % n\n",
    "\n",
    "    return series.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "#Genero due serie bootrsap indipendenti, calcolo sharpe e faccio differenza, calcolo p (prob che WMJ NON sia migliore) e siamo a posto\n",
    "#                   ATTENZIONE QUA DENTROOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "def bootstrap_sharpe_comparison_stationary(\n",
    "    returns1, returns2, num_bootstrap_samples=2000,\n",
    "    block_size=6, risk_free_rate=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Confronta gli Sharpe Ratio usando lo Stationary Bootstrap (Politis & Romano).\n",
    "    \"\"\"\n",
    "    sharpes_diff = []\n",
    "    n = len(returns1)\n",
    "\n",
    "    # p = probabilit√† di aprire un nuovo blocco\n",
    "    p = 1 / block_size   # CONSIGLIATO in letteratura\n",
    "\n",
    "    for _ in range(num_bootstrap_samples):\n",
    "\n",
    "        # --- Stationary bootstrap sample ---\n",
    "        sample1 = stationary_bootstrap_series(returns1, p)          # Cosi serie indipendenti, portafogli costruiti su stesso perioo immagino siano dipendenti? \n",
    "        sample2 = stationary_bootstrap_series(returns2, p)\n",
    "        # Calcolo Sharpe su serie bootstrappate\n",
    "        sharpe1 = calculate_sharpe_ratio(sample1, risk_free_rate)\n",
    "        sharpe2 = calculate_sharpe_ratio(sample2, risk_free_rate)\n",
    "\n",
    "        if not np.isnan(sharpe1) and not np.isnan(sharpe2):\n",
    "            sharpes_diff.append(sharpe1 - sharpe2)\n",
    "\n",
    "    if not sharpes_diff:\n",
    "        return np.nan\n",
    "\n",
    "    # p-value unilaterale: P(Sharpe1 <= Sharpe2)\n",
    "    sharpes_diff = np.array(sharpes_diff)\n",
    "    p_value_one_sided = np.mean(sharpes_diff <= 0)\n",
    "\n",
    "    return p_value_one_sided\n",
    "\n",
    "'''\n",
    "Fine Bootstrap di Romano (stazionario)\n",
    "'''\n",
    "\n",
    "# ******************************************\n",
    "# Applicazione del bootstrap su P10 - VW **\n",
    "# ******************************************\n",
    "\n",
    "# --- Applicazione ---\n",
    "print(\"\\n--- Confronto Statistico degli Sharpe Ratio basandoci su P10 (WMJ vs Altri VW) ---\")\n",
    "# Tasso risk-free mensile (assumo 0 per semplicit√† ) --> giustificare durante tesi\n",
    "RISK_FREE_MONTHLY = 0.0                                                                             # --> ATTENZIONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "wmj_port_name = best_wportfolios['wmj']['portafoglio']\n",
    "wmj_returns_series = weighted_portfolios['wmj'][wmj_port_name]\n",
    "\n",
    "for metodo in [k for k in weighted_portfolios.keys() if k != 'wmj']:\n",
    "    other_port_name = best_wportfolios[metodo]['portafoglio']\n",
    "    other_returns_series = weighted_portfolios[metodo][other_port_name]\n",
    "\n",
    "    min_len = min(len(wmj_returns_series), len(other_returns_series))\n",
    "    wmj_sample_returns = wmj_returns_series.iloc[:min_len]\n",
    "    other_sample_returns = other_returns_series.iloc[:min_len]\n",
    "\n",
    "    # Chiamo la funzione (funzione vecchia)\n",
    "    # p_value = bootstrap_sharpe_comparison_block(wmj_sample_returns, other_sample_returns, num_bootstrap_samples=2000, risk_free_rate=RISK_FREE_MONTHLY)\n",
    "\n",
    "    #funzione di Romano\n",
    "    p_value = bootstrap_sharpe_comparison_stationary(wmj_sample_returns, other_sample_returns, num_bootstrap_samples=2000, block_size=6, risk_free_rate=RISK_FREE_MONTHLY)\n",
    "\n",
    "    # Interpretazione del p-value   \n",
    "    alpha = 0.05 # Livello di significativit√†\n",
    "\n",
    "    print(f\"\\nConfronto WMJ vs {metodo.upper()} - P10:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"La performance risk-adjusted di WMJ √® STATISTICAMENTE SUPERIORE a quella di {metodo.upper()} (p-value = {p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"Non c'√® una differenza statisticamente significativa nella performance risk-adjusted tra WMJ e {metodo.upper()} (p-value = {p_value:.4f})\")\n",
    "\n",
    "\n",
    "# ******************************************\n",
    "# Applicazione del bootstrap su LS - VW **\n",
    "# ******************************************\n",
    "\n",
    "# --- Applicazione ---\n",
    "print(\"\\n--- Confronto Statistico degli Sharpe Ratio basandoci su LS (WMJ vs Altri VW) ---\")\n",
    "# Tasso risk-free mensile (assumo 0 per semplicit√† ) --> giustificare durante tesi\n",
    "RISK_FREE_MONTHLY = 0.0\n",
    "\n",
    "wmj_port_name = best_wportfolios_WLS['wmj']['portafoglio']\n",
    "wmj_returns_series = weighted_portfolios['wmj'][wmj_port_name]\n",
    "\n",
    "for metodo in [k for k in weighted_portfolios.keys() if k != 'wmj']:\n",
    "    other_port_name = best_wportfolios_WLS[metodo]['portafoglio']\n",
    "    other_returns_series = weighted_portfolios[metodo][other_port_name]\n",
    "\n",
    "    min_len = min(len(wmj_returns_series), len(other_returns_series))\n",
    "    wmj_sample_returns = wmj_returns_series.iloc[:min_len]\n",
    "    other_sample_returns = other_returns_series.iloc[:min_len]\n",
    "\n",
    "    # Chiamo la funzione (vecchia)\n",
    "    # p_value = bootstrap_sharpe_comparison_block(wmj_sample_returns, other_sample_returns, num_bootstrap_samples=2000, risk_free_rate=RISK_FREE_MONTHLY)\n",
    "\n",
    "    #funzione di Romano\n",
    "    p_value = bootstrap_sharpe_comparison_stationary(wmj_sample_returns, other_sample_returns, num_bootstrap_samples=2000, block_size=6, risk_free_rate=RISK_FREE_MONTHLY)\n",
    "\n",
    "    # Interpretazione del p-value   \n",
    "    alpha = 0.05 # Livello di significativit√†\n",
    "\n",
    "    print(f\"\\nConfronto WMJ vs {metodo.upper()} - LS:\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"La performance risk-adjusted di WMJ √® STATISTICAMENTE SUPERIORE a quella di {metodo.upper()} (p-value = {p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"Non c'√® una differenza statisticamente significativa nella performance risk-adjusted tra WMJ e {metodo.upper()} (p-value = {p_value:.4f})\")\n",
    "\n",
    "\n",
    "# Dai risultati, wmj √® tra i portafogli pi√π efficienti (ottimo trade-off rendimento/rischio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e53eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== TEST STATISTICO SU RENDIMENTI ==========================\n",
    "\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "t-test se dati normali\n",
    "Wilcoxon se non normali\n",
    "Cohens'd piccolo --> questo significa che, sebbene WMJ sia statisticamente superiore, la dimensione pratica della differenza non √® enorme 9tipico dei dati finanziari che sono rumorosi)\n",
    "Correzione Holm per test multipli (meno conservativa di Bonferroni) --> riduce falsi positivi mantenendo potenza\n",
    "\n",
    "W --> wilcoxon statisticamente significativo\n",
    "w --> wilconox non significativo\n",
    "T --> t-test statisticamente significativo\n",
    "t --> t-test non significativo\n",
    "‚úÖ --> test raccomandato risultato significativo\n",
    "\n",
    "Differenze non significative in LS perch√© portafogli LS tendono a ridurre il rischio e livellare i rendimenti, perch√© il long e il short si compensano parzialmente.\n",
    "    Questo spesso diminuisce le differenze medie tra WMJ e gli altri metodi, quindi il test statistico fatica a trovare significativit√†.\n",
    "'''\n",
    "\n",
    "def comprehensive_comparison_test(series_a, series_b, name_a=\"WMJ\", name_b=\"OTHER\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    Esegue test sia parametrici che non-parametrici per confrontare due serie.\n",
    "    \"\"\"\n",
    "    # Allineamento\n",
    "    idx = series_a.index.intersection(series_b.index)\n",
    "    a = series_a.loc[idx].dropna()\n",
    "    b = series_b.loc[idx].dropna()\n",
    "    idx = a.index.intersection(b.index)\n",
    "    a = a.loc[idx]\n",
    "    b = b.loc[idx]\n",
    "    \n",
    "    if len(a) < 10:\n",
    "        return {\n",
    "            'n_observations': len(a),\n",
    "            'warning': f\"Troppo poche osservazioni ({len(a)})\"\n",
    "        }\n",
    "    \n",
    "    # Calcola differenze\n",
    "    differences = a - b\n",
    "    \n",
    "    # Test di normalit√†\n",
    "    _, normality_p = shapiro(differences) if len(differences) >= 3 else (None, None)\n",
    "    is_normal = normality_p > 0.05 if normality_p is not None else False\n",
    "    \n",
    "    # T-test parametrico\n",
    "    t_stat, t_p = ttest_rel(a, b)\n",
    "    \n",
    "    # Test non-parametrico di Wilcoxon (pi√π robusto)\n",
    "    try:\n",
    "        w_stat, w_p = wilcoxon(differences, alternative='two-sided')\n",
    "    except:\n",
    "        w_stat, w_p = np.nan, np.nan\n",
    "    \n",
    "    # Statistiche descrittive\n",
    "    mean_a = a.mean()\n",
    "    mean_b = b.mean()\n",
    "    mean_diff = differences.mean()\n",
    "    median_diff = differences.median()\n",
    "    std_diff = differences.std()\n",
    "    \n",
    "    # Effect size (Cohen's d) --> VECCHIO, UTILE PER CAMPIONI INDIPENDENTI (I NOSTRI LO SONO????)\n",
    "    '''\n",
    "    pooled_std = np.sqrt((a.var() + b.var()) / 2)\n",
    "    cohens_d = mean_diff / pooled_std if pooled_std > 0 else np.nan\n",
    "    '''\n",
    "\n",
    "    # Nuovo Cohen che tiene conto di paired samples\n",
    "    std_diff = differences.std(ddof=1)  # deviazione standard della differenza\n",
    "    cohens_d = differences.mean() / std_diff\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'n_observations': len(a),\n",
    "        'mean_a': mean_a,\n",
    "        'mean_b': mean_b,\n",
    "        'mean_difference': mean_diff,\n",
    "        'median_difference': median_diff,\n",
    "        'std_difference': std_diff,\n",
    "        'cohens_d': cohens_d,\n",
    "        'normality_p': normality_p,\n",
    "        'is_normal': is_normal,\n",
    "        't_statistic': t_stat,\n",
    "        't_p_value': t_p,\n",
    "        'wilcoxon_statistic': w_stat,\n",
    "        'wilcoxon_p_value': w_p,\n",
    "        't_significant': t_p < alpha,\n",
    "        'wilcoxon_significant': w_p < alpha,\n",
    "        'recommended_test': 'wilcoxon' if not is_normal else 't_test'\n",
    "    }\n",
    "\n",
    "def perform_multiple_comparisons_analysis(wmj_returns, other_portfolios, alpha=0.05, \n",
    "                                        correction_method='holm'):\n",
    "    \"\"\"\n",
    "    Esegue test multipli con correzione per confronti multipli.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    methods = [m for m in other_portfolios.keys() if m != 'wmj']\n",
    "    \n",
    "    # Raccogli tutti i p-value\n",
    "    t_p_values = []\n",
    "    w_p_values = []\n",
    "    \n",
    "    for method_name in methods:\n",
    "        other_returns = other_portfolios[method_name]\n",
    "        result = comprehensive_comparison_test(\n",
    "            wmj_returns, other_returns, \"WMJ\", method_name.upper(), alpha\n",
    "        )\n",
    "        results[method_name] = result\n",
    "        \n",
    "        if 'warning' not in result:\n",
    "            t_p_values.append(result['t_p_value'])\n",
    "            w_p_values.append(result['wilcoxon_p_value'])\n",
    "        else:\n",
    "            t_p_values.append(1.0)  # Non significativo per metodi con warning\n",
    "            w_p_values.append(1.0)\n",
    "    \n",
    "    # Correzione per test multipli\n",
    "    t_corrected = multipletests(t_p_values, alpha=alpha, method=correction_method)\n",
    "    w_corrected = multipletests(w_p_values, alpha=alpha, method=correction_method)\n",
    "    \n",
    "    # Aggiungi correzioni ai risultati\n",
    "    for i, method_name in enumerate(methods):\n",
    "        if 'warning' not in results[method_name]:\n",
    "            results[method_name]['t_p_corrected'] = t_corrected[1][i]\n",
    "            results[method_name]['wilcoxon_p_corrected'] = w_corrected[1][i]\n",
    "            results[method_name]['t_significant_corrected'] = t_corrected[0][i]\n",
    "            results[method_name]['wilcoxon_significant_corrected'] = w_corrected[0][i]\n",
    "    \n",
    "    return results, correction_method\n",
    "\n",
    "def print_comprehensive_results(results, title, correction_method=None, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Stampa risultati comprensivi con raccomandazioni.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Header\n",
    "    header = f\"{'Metodo':<12} {'n':<4} {'T-test':<8} {'Wilcoxon':<9} {'Raccomandato':<12} {'Effect Size':<11} {'Note'}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    significant_methods = []\n",
    "    \n",
    "    for method, result in results.items():\n",
    "        if 'warning' in result:\n",
    "            print(f\"{method.upper():<12} {result['n_observations']:<4} {'N/A':<8} {'N/A':<9} {'N/A':<12} {'N/A':<11} {result['warning']}\")\n",
    "            continue\n",
    "        \n",
    "        method_label = method.upper()\n",
    "        \n",
    "        # Scegli il test raccomandato\n",
    "        if result['recommended_test'] == 'wilcoxon':\n",
    "            main_p = result['wilcoxon_p_corrected'] if correction_method else result['wilcoxon_p_value']\n",
    "            is_significant = main_p < alpha\n",
    "            test_symbol = \"W\" if is_significant else \"w\"\n",
    "        else:\n",
    "            main_p = result['t_p_corrected'] if correction_method else result['t_p_value']\n",
    "            is_significant = main_p < alpha\n",
    "            test_symbol = \"T\" if is_significant else \"t\"\n",
    "        \n",
    "        # Effect size interpretation\n",
    "        d = abs(result['cohens_d'])\n",
    "        if d >= 0.8:\n",
    "            effect = \"Grande\"\n",
    "        elif d >= 0.5:\n",
    "            effect = \"Medio\"  \n",
    "        elif d >= 0.2:\n",
    "            effect = \"Piccolo\"\n",
    "        else:\n",
    "            effect = \"Trascurabile\"\n",
    "        \n",
    "        # Note\n",
    "        note = \"\"\n",
    "        if not result['is_normal']:\n",
    "            note += \"Non-norm\"\n",
    "        if result['mean_difference'] > 0:\n",
    "            note += \" WMJ>\" + method_label[:3]\n",
    "        else:\n",
    "            note += \" WMJ<\" + method_label[:3]\n",
    "        \n",
    "        t_p_display = f\"{result['t_p_value']:.4f}\"\n",
    "        w_p_display = f\"{result['wilcoxon_p_value']:.4f}\"\n",
    "        sig_symbol = \"‚úÖ\" if is_significant else \"‚ûñ\"\n",
    "        \n",
    "        print(f\"{method_label:<12} {result['n_observations']:<4} {t_p_display:<8} {w_p_display:<9} \"\n",
    "              f\"{test_symbol+sig_symbol:<12} {effect:<11} {note}\")\n",
    "        \n",
    "        if is_significant:\n",
    "            significant_methods.append(method_label)\n",
    "    \n",
    "    print(f\"\\nüéØ WMJ significativamente superiore a: {significant_methods}\")\n",
    "    print(f\"üìä Test raccomandato: Wilcoxon (dati non-normali)\")\n",
    "    if correction_method:\n",
    "        print(f\"üîß Correzione applicata: {correction_method}\")\n",
    "\n",
    "# ==================== APPLICAZIONE ======================\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# P10 Analysis\n",
    "print(\"ANALISI STATISTICA ROBUSTA: P10 VW\")\n",
    "wmj_ret_p10 = weighted_portfolios['wmj'][best_wportfolios['wmj']['portafoglio']]\n",
    "other_portfolios_p10 = {\n",
    "    method: weighted_portfolios[method][best_wportfolios[method]['portafoglio']]\n",
    "    for method in weighted_portfolios.keys() if method != 'wmj'\n",
    "}\n",
    "\n",
    "results_p10, correction_method = perform_multiple_comparisons_analysis(\n",
    "    wmj_ret_p10, other_portfolios_p10, alpha, 'holm'\n",
    ")\n",
    "print_comprehensive_results(results_p10, \"P10 VW: WMJ vs Altri Metodi (Test Robusti)\", correction_method, alpha)\n",
    "\n",
    "# LS Analysis  \n",
    "print(\"\\n\\nANALISI STATISTICA ROBUSTA: LS VW\")\n",
    "wmj_ret_ls = weighted_portfolios['wmj'][best_wportfolios_WLS['wmj']['portafoglio']]\n",
    "other_portfolios_ls = {\n",
    "    method: weighted_portfolios[method][best_wportfolios_WLS[method]['portafoglio']]\n",
    "    for method in weighted_portfolios.keys() if method != 'wmj'\n",
    "}\n",
    "\n",
    "results_ls, correction_method = perform_multiple_comparisons_analysis(\n",
    "    wmj_ret_ls, other_portfolios_ls, alpha, 'holm'\n",
    ")\n",
    "print_comprehensive_results(results_ls, \"LS VW: WMJ vs Altri Metodi (Test Robusti)\", correction_method, alpha)\n",
    "\n",
    "# Summary insight\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üìà RIEPILOGO INSIGHTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚Ä¢ I rendimenti finanziari raramente seguono distribuzione normale\")\n",
    "print(\"‚Ä¢ Il test di Wilcoxon √® pi√π appropriato per questi dati\") \n",
    "print(\"‚Ä¢ La correzione di Holm controlla il tasso di errore di Tipo I nei test multipli\")\n",
    "print(\"‚Ä¢ L'effect size (Cohen's d) quantifica la dimensione pratica delle differenze\")\n",
    "print(\"‚Ä¢ WMJ sembra performare meglio nella strategia P10 che in Long-Short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== COSTI DI TRANSAZIONE ==========================\n",
    "\n",
    "def comprehensive_transaction_cost_analysis_extended():\n",
    "    \"\"\"\n",
    "    Analisi completa dei costi di transazione per P10 e Long-Short\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Raccogli turnover e rendimenti per P10 VW\n",
    "    turnover_data_p10 = {}\n",
    "    gross_returns_p10 = {}\n",
    "    \n",
    "    for method in weighted_portfolios.keys():\n",
    "        if method in portfolios_stock_reallocation:\n",
    "            df_turn = portfolios_stock_reallocation[method]['VW_turnover']\n",
    "            turnover_p10 = df_turn[df_turn['portfolio'] == 10.0]['VW_turnover'].values[0]\n",
    "            turnover_data_p10[method] = turnover_p10\n",
    "            gross_returns_p10[method] = weighted_portfolios[method]['port10'].mean()\n",
    "    \n",
    "    # 2. Raccogli turnover e rendimenti per Long-Short VW\n",
    "    # Per LS assumiamo che il turnover sia la media tra P1 e P10 (o usa metrica specifica se disponibile)\n",
    "    turnover_data_ls = {}\n",
    "    gross_returns_ls = {}\n",
    "    \n",
    "    for method in weighted_portfolios.keys():\n",
    "        if method in portfolios_stock_reallocation:\n",
    "            df_turn = portfolios_stock_reallocation[method]['VW_turnover']\n",
    "            # Turnover LS = media ponderata tra P1 e P10 (o puoi usare metrica diversa)\n",
    "            turnover_p1 = df_turn[df_turn['portfolio'] == 1.0]['VW_turnover'].values[0]\n",
    "            turnover_p10 = df_turn[df_turn['portfolio'] == 10.0]['VW_turnover'].values[0]\n",
    "            turnover_data_ls[method] = (turnover_p1 + turnover_p10) / 2  # Media semplice\n",
    "            gross_returns_ls[method] = weighted_portfolios[method]['long_short'].mean()\n",
    "    \n",
    "    # 3. Range di costi di transazione realistici\n",
    "    transaction_costs = [0.001, 0.002, 0.005, 0.01, 0.015, 0.02]  # 0.1% - 2%\n",
    "    \n",
    "    # 4. Calcola rendimenti netti per P10\n",
    "    results_p10 = []\n",
    "    for cost in transaction_costs:\n",
    "        for method in turnover_data_p10.keys():\n",
    "            gross_ret = gross_returns_p10[method]\n",
    "            turnover = turnover_data_p10[method]\n",
    "            \n",
    "            cost_impact = turnover * cost\n",
    "            net_return = gross_ret - cost_impact\n",
    "            \n",
    "            gross_series = weighted_portfolios[method]['port10']\n",
    "            net_series = gross_series - cost_impact\n",
    "            net_sharpe = calculate_sharpe_ratio(net_series)\n",
    "            \n",
    "            results_p10.append({\n",
    "                'Strategy': 'P10',\n",
    "                'Method': method.upper(),\n",
    "                'Transaction_Cost': cost,\n",
    "                'Turnover': turnover,\n",
    "                'Gross_Return': gross_ret,\n",
    "                'Cost_Impact': cost_impact,\n",
    "                'Net_Return': net_return,\n",
    "                'Net_Sharpe': net_sharpe,\n",
    "                'Return_Loss_pct': (cost_impact / gross_ret) * 100 if gross_ret != 0 else 0\n",
    "            })\n",
    "    \n",
    "    # 5. Calcola rendimenti netti per Long-Short\n",
    "    results_ls = []\n",
    "    for cost in transaction_costs:\n",
    "        for method in turnover_data_ls.keys():\n",
    "            gross_ret = gross_returns_ls[method]\n",
    "            turnover = turnover_data_ls[method]\n",
    "            \n",
    "            cost_impact = turnover * cost\n",
    "            net_return = gross_ret - cost_impact\n",
    "            \n",
    "            gross_series = weighted_portfolios[method]['long_short']\n",
    "            net_series = gross_series - cost_impact\n",
    "            net_sharpe = calculate_sharpe_ratio(net_series)\n",
    "            \n",
    "            results_ls.append({\n",
    "                'Strategy': 'Long-Short',\n",
    "                'Method': method.upper(),\n",
    "                'Transaction_Cost': cost,\n",
    "                'Turnover': turnover,\n",
    "                'Gross_Return': gross_ret,\n",
    "                'Cost_Impact': cost_impact,\n",
    "                'Net_Return': net_return,\n",
    "                'Net_Sharpe': net_sharpe,\n",
    "                'Return_Loss_pct': (cost_impact / gross_ret) * 100 if gross_ret != 0 else 0\n",
    "            })\n",
    "    \n",
    "    # 6. Combina i risultati\n",
    "    all_results = results_p10 + results_ls\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Esegui analisi estesa\n",
    "cost_analysis_extended = comprehensive_transaction_cost_analysis_extended()\n",
    "\n",
    "# 7. Ranking separato per P10 e Long-Short\n",
    "def print_strategy_rankings(df, strategy_name, costs_to_show=[0.001, 0.005, 0.01, 0.02]):\n",
    "    print(f\"\\nüèÜ RANKING {strategy_name.upper()} - RENDIMENTO NETTO:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    strategy_data = df[df['Strategy'] == strategy_name]\n",
    "    \n",
    "    for cost in costs_to_show:\n",
    "        subset = strategy_data[strategy_data['Transaction_Cost'] == cost].copy()\n",
    "        subset = subset.sort_values('Net_Return', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüìä Costo transazione: {cost*100:.1f}%\")\n",
    "        print(f\"{'Rank':<4} {'Method':<12} {'Net Return':<12} {'Turnover':<10} {'Loss %':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (_, row) in enumerate(subset.head(5).iterrows(), 1):\n",
    "            print(f\"{i:<4} {row['Method']:<12} {row['Net_Return']:.4f}      {row['Turnover']:.4f}    {row['Return_Loss_pct']:.2f}%\")\n",
    "        \n",
    "        # Posizione di WMJ\n",
    "        wmj_subset = subset[subset['Method'] == 'WMJ']\n",
    "        if not wmj_subset.empty:\n",
    "            wmj_pos = subset.reset_index().query(\"Method == 'WMJ'\").index[0] + 1\n",
    "            print(f\"üéØ WMJ Position: #{wmj_pos}\")\n",
    "\n",
    "# Stampa ranking per entrambe le strategie\n",
    "print_strategy_rankings(cost_analysis_extended, 'P10')\n",
    "print_strategy_rankings(cost_analysis_extended, 'Long-Short')\n",
    "\n",
    "# 8. Analisi break-even comparativa\n",
    "def break_even_analysis(df, strategy_name):\n",
    "    print(f\"\\n\\nüí∞ BREAK-EVEN ANALYSIS - {strategy_name.upper()}:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    strategy_data = df[df['Strategy'] == strategy_name]\n",
    "    wmj_data = strategy_data[strategy_data['Method'] == 'WMJ']\n",
    "    \n",
    "    if wmj_data.empty:\n",
    "        print(f\"‚ö†Ô∏è WMJ non trovato per {strategy_name}\")\n",
    "        return\n",
    "    \n",
    "    wmj_gross = wmj_data['Gross_Return'].iloc[0]\n",
    "    wmj_turnover = wmj_data['Turnover'].iloc[0]\n",
    "    \n",
    "    print(f\"WMJ Turnover: {wmj_turnover:.4f}\")\n",
    "    print(f\"WMJ Gross Return: {wmj_gross:.4f}\")\n",
    "    \n",
    "    competitors = ['WTOPSIS', 'WVIKOR', 'WPROMETHEE', 'DOLVOL6', 'GP', 'ILL6']\n",
    "    unique_methods = strategy_data['Method'].unique()\n",
    "    \n",
    "    for comp in competitors:\n",
    "        if comp in unique_methods:\n",
    "            comp_data = strategy_data[strategy_data['Method'] == comp]\n",
    "            comp_gross = comp_data['Gross_Return'].iloc[0]\n",
    "            comp_turnover = comp_data['Turnover'].iloc[0]\n",
    "            \n",
    "            if abs(comp_turnover - wmj_turnover) > 1e-6:\n",
    "                breakeven_cost = (comp_gross - wmj_gross) / (comp_turnover - wmj_turnover)\n",
    "                if breakeven_cost > 0:\n",
    "                    print(f\"vs {comp}: Break-even cost = {breakeven_cost*100:.2f}%\")\n",
    "                else:\n",
    "                    print(f\"vs {comp}: WMJ dominante (sempre superiore)\")\n",
    "\n",
    "# Esegui break-even per entrambe\n",
    "break_even_analysis(cost_analysis_extended, 'P10')\n",
    "break_even_analysis(cost_analysis_extended, 'Long-Short')\n",
    "\n",
    "# 10. Tabelle finali comparative\n",
    "def create_summary_table(df, strategy_name):\n",
    "    strategy_data = df[df['Strategy'] == strategy_name]\n",
    "    summary_costs = strategy_data[strategy_data['Transaction_Cost'].isin([0.001, 0.005, 0.01])]\n",
    "    \n",
    "    pivot_table = summary_costs.pivot(index='Method', columns='Transaction_Cost', values='Net_Return')\n",
    "    pivot_table.columns = [f'Cost_{c*100:.1f}%' for c in pivot_table.columns]\n",
    "    pivot_table = pivot_table.sort_values('Cost_0.5%', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìã SUMMARY TABLE - {strategy_name.upper()} Net Returns by Transaction Cost:\")\n",
    "    print(pivot_table.round(4))\n",
    "    \n",
    "    return pivot_table\n",
    "\n",
    "# Crea tabelle per entrambe le strategie\n",
    "summary_p10 = create_summary_table(cost_analysis_extended, 'P10')\n",
    "summary_ls = create_summary_table(cost_analysis_extended, 'Long-Short')\n",
    "\n",
    "# 11. Confronto diretto P10 vs Long-Short per WMJ\n",
    "print(\"\\nüîç WMJ: P10 vs Long-Short Performance\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "wmj_comparison = cost_analysis_extended[cost_analysis_extended['Method'] == 'WMJ']\n",
    "for cost in [0.001, 0.005, 0.01, 0.02]:\n",
    "    cost_data = wmj_comparison[wmj_comparison['Transaction_Cost'] == cost]\n",
    "    p10_row = cost_data[cost_data['Strategy'] == 'P10']\n",
    "    ls_row = cost_data[cost_data['Strategy'] == 'Long-Short']\n",
    "    \n",
    "    if not p10_row.empty and not ls_row.empty:\n",
    "        p10_net = p10_row['Net_Return'].iloc[0]\n",
    "        ls_net = ls_row['Net_Return'].iloc[0]\n",
    "        p10_turnover = p10_row['Turnover'].iloc[0]\n",
    "        ls_turnover = ls_row['Turnover'].iloc[0]\n",
    "        \n",
    "        print(f\"\\nCosto {cost*100:.1f}%:\")\n",
    "        print(f\"  P10:        Net Return = {p10_net:.4f}, Turnover = {p10_turnover:.4f}\")\n",
    "        print(f\"  Long-Short: Net Return = {ls_net:.4f}, Turnover = {ls_turnover:.4f}\")\n",
    "        print(f\"  Differenza: {ls_net - p10_net:.4f} {'(LS > P10)' if ls_net > p10_net else '(P10 > LS)'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
